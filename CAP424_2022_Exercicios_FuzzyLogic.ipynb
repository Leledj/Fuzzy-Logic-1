{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lourencocavalcante/Fuzzy-Logic/blob/main/CAP424_2022_Exercicios_FuzzyLogic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCGrfcaNk9Ju"
      },
      "source": [
        "[<img src=\"https://github.com/lourencocavalcante/LogosINPE/blob/main/logoinpe.png?raw=true\" width = 500 align=\"left\">](https://www.gov.br/inpe/pt-br)\n",
        "\n",
        "[<img src=\"https://github.com/lourencocavalcante/LogosINPE/blob/main/LogoCAP.png?raw=true\" width = 300 align=\"right\">](http://www.inpe.br/posgraduacao/cap/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ect6m7UiI28E"
      },
      "source": [
        "# <span style=\"color:#336699; text-align: center;\">Trabalho Final da disciplina de Lógica Nebulosa (CAP-424)</span>\n",
        "<hr style=\"border:2px solid #0077b9;\">\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"text-align: center;font-size: 110%;\">\n",
        "    <b>CAP-424 – Lógica Nebulosa (Fuzzy Logic)</b>\n",
        "    <br/>\n",
        "    <b>Docente:</b> Dr. Lamartine Nogueira Frutuoso Guimarães\n",
        "    <br/><br/>\n",
        "    <b>Discente:</b> Lourenço José Cavalcante Neto\n",
        "    <br/>\n",
        "    <b>E-mail:</b> <a href=\"mailto:lourenco.cavalcante@ifto.edu.b\">lourenco.cavalcante@ifto.edu.br</a>\n",
        "\n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"text-align: justify;  margin-left: 20%; margin-right: 20%;\">\n",
        "<b>Objetivo: </b> O principal objetivo deste caderno notebook é apresentar as questões propostas pelo professor em torno das discussões ocorridas durante a disciplina de Lógica Nebulosa (CAP-424) e suas respectivas soluções, como trabalho final da disciplina.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFcD5Gx3Vuyz"
      },
      "source": [
        "<span style=\"text-align: center;\"> Este notebook também pode ser visualizado no meu diretório de materiais e atividades da disciplina **CAP-324** no **Github**,  [Clicando aqui](https://github.com/lourencocavalcante/Fuzzy-Logic.git) </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lógica Nebulosa (Fuzzy Logic)**"
      ],
      "metadata": {
        "id": "66PyRnagA772"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Okfq4RbVmXD"
      },
      "source": [
        "Na lógica convencional (binária), um elemento pertence ou não pertence a um determinado conjunto, e nunca se encontra entre estes dois estados possíveis. Esta é uma maneira de simplificar um mundo inerentemente complexo, mas - argumentam os defensores da lógica nebulosa - esta simplificação acaba por distorcer a realidade (GRINT, 1997). A lógica nebulosa é um método que permite expressar incertezas de maneira mais consistente, através dos conjuntos nebulosos: ao invés de simplesmente pertencer ou não pertencer, um elemento poderá ter vários graus de pertinência a um conjunto. \n",
        "\n",
        "O conceito de lógica nebulosa (fuzzy logic ou LN) foi introduzido por Lotfi Zadeh em 1965, como uma forma de reduzir e explicar a complexidade de sistemas (Cox, 1998). Esta idéia permaneceu praticamente desconhecida pelo grande público até o final da década de 1980, quando o metrô de Sendai adotou um sistema baseado na LN - o Automatic Train Operator (ATO) - e surgiram várias empresas que tinham o objetivo de desenvolver e comercializar produtos baseados nesta tecnologia. Apesar de o interesse comercial haver arrefecido, hoje é possível encontrar aplicações da lógica nebulosa em áreas bem diferentes daquela em que surgiu\n",
        "\n",
        "Os conjuntos nebulosos (fuzzy sets) são funções que mapeiam, em uma escala de zero a um, esta pertinência de um determinado elemento ao conjunto. O valor zero indica que o elemento não pertence ao conjunto, enquanto o valor um significa que o elemento é completamente representativo do conjunto; valores entre estes dois indicam graus intermediários de pertinência.\n",
        "\n",
        "Com conjuntos nebulosos é possível realizar várias operações - as básicas são interseção, união e complemento - e, com regras de inferência (policies), criar modelos que auxiliem na tomada de decisão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IUoRbDBTZZj"
      },
      "source": [
        "## **Importação de módulos e bibliotecas**\n",
        "São várias as bibliotecas que podem ser utilizadas para realizar o pré-processamento de dados. Entre elas podemos destacar a biblioteca **Pandas**. Trata-se de uma biblioteca para leitura, manipulação e análise de dados tabulados. Essa biblioteca oferece estruturas de dados e operações para manipular conjuntos massivos de tabelas numéricas e séries temporais de forma otimizada. No python, por convensão, as bibliotecas são importadas conforme podemos ver na célula abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR79xHB6ULVt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import math\n",
        "import calendar\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwRSFhwpcAkb"
      },
      "source": [
        "## **Obtendo os arquivos de dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5eAC57TcOF9"
      },
      "outputs": [],
      "source": [
        "#Vamos baixar os dados do Google Drive para o Notebook\n",
        "\n",
        "!gdown --id 1DflICrG3vlewOXnLYrsEqhK0WFn2Bz0U #Down. WD_campina_report_RZLwcDmDtNdrop_b2.1_report\n",
        "!gdown --id 1HF3z0QjMhbOMMzvzhGLHmVHg0D6GQx5d #Down. level1_output_sfc\n",
        "!gdown --id 1exlLcdlPzLOPhzC9l97TNlMnysEH0AWk #Down. level1_output_bt\n",
        "!gdown --id 1suHWSYCt_3066gmPylMU9un9owYI3yVD #Down. level2_output_cld_atto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8o27PICcTgz"
      },
      "source": [
        "Temos quatro tipos de arquivos:\n",
        "\n",
        "* **level1_output_sfc.txt**: \n",
        "`Neste arquivo estão os dados do ambiente próximo à superfície.`\n",
        "\n",
        "* **level1_output_bt.txt**: \n",
        "`Neste arquivos estão os dados das medidas dos canais do microonda.`\n",
        "\n",
        "* **level2_output_cld_atto.txt**: \n",
        "`Neste arquivo estão as variáveis estimadas associadas a parâmetros de nuvens.`\n",
        "\n",
        "* **JWD_campina_report_RZLwcDmDtNdrop_b2.1_report.txt**: \n",
        "`Neste arquivo estão dados de medida da distribuição das gotas de chuva que chegam à superfície.`\n",
        "\n",
        "Com o objetivo de facilitar a leitura dos dados e torná-los **tidy**, primeiramente será necessário carregá-los como um **dataframe**. Vamos carregar os dados e criar os dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEKpBOUGdBaM"
      },
      "outputs": [],
      "source": [
        "#Abrindo os dados e criando os Dataframes\n",
        "df_rd80_joss = pd.read_table('./JWD_campina_report_RZLwcDmDtNdrop_b2.1_report.txt',header=None, delim_whitespace=True)\n",
        "df_mp3000A_1 = pd.read_table('./level1_output_sfc.txt',header=None,delim_whitespace=True)\n",
        "df_mp3000A_2 = pd.read_table('./level1_output_bt.txt',header=None,delim_whitespace=True)\n",
        "df_mp3000A_3 = pd.read_table('./level2_output_cld_atto.txt',header=None,delim_whitespace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX5hfsI4eQyr"
      },
      "source": [
        "Vamos criar uma lista contendo os Dataframes para que possamos manipulá-los a partir daqui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCjPBFC6eZSM"
      },
      "outputs": [],
      "source": [
        "list_dataframes = [df_rd80_joss, df_mp3000A_1, df_mp3000A_2, df_mp3000A_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk3dDYL8fN3b"
      },
      "outputs": [],
      "source": [
        "#Vamos verificar o tamanho dos Datasets e visualizar as suas colunas.\n",
        "print('TAMANHO DO DATASET E QUANTIDADE DE ATRIBUTOS:\\n')\n",
        "for item in list_dataframes:\n",
        "  print('Tamanho do dataset: ',item.shape[0], ' - Quantidade de atributos: ', item.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRCfkvYfqJT"
      },
      "source": [
        "Agora vamos investigar mais algumas informações sobre nossos dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdHwXElmfyjz"
      },
      "outputs": [],
      "source": [
        "for itemDtypes in list_dataframes:\n",
        "  print(itemDtypes.info(),'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtQzcuR4gJTN"
      },
      "source": [
        "Vamos visualizar as 3 primeiras e 3 últimas linhas de cada arquivo de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "damGIqqggOrD"
      },
      "outputs": [],
      "source": [
        "#Visualização das primeiras 3 linhas e últimas 3 linhas\n",
        "df_rd80_joss.head(n=3).append(df_rd80_joss.tail(n=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_DT89F4gYu8"
      },
      "outputs": [],
      "source": [
        "#Visualização das primeiras 3 linhas e últimas 3 linhas\n",
        "df_mp3000A_1.head(n=3).append(df_mp3000A_1.tail(n=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JTCNohDgZis"
      },
      "outputs": [],
      "source": [
        "#Visualização das primeiras 3 linhas e últimas 3 linhas\n",
        "df_mp3000A_2.head(n=3).append(df_mp3000A_2.tail(n=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rLEcAU2gaMM"
      },
      "outputs": [],
      "source": [
        "#Visualização das primeiras 3 linhas e últimas 3 linhas\n",
        "df_mp3000A_3.head(n=3).append(df_mp3000A_3.tail(n=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDumsiE8hND0"
      },
      "source": [
        "Como vimos, a colunas de dados não vieram com seus nomes. Vamos resolver isso adicionando nome para as colunas dos dados do instrumento RD80 e do MP3000A, uma vez que sabemos o que cada coluna representa, de acordo com as especificações recebidas juntamente com os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyUbG9wxhUoz"
      },
      "outputs": [],
      "source": [
        "#Vamos adicionar os títulos das colunas:\n",
        "df_rd80_joss.columns = ['Ano', 'Mes', 'dia', 'H', 'M', 'S','Rain_Intensity_mm_h', 'radar_reflectivity_1_mm6m3','Liquid_watercontent_g_m3',\n",
        "'Mean_weight_diameter_mm', 'Time_integration_s', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D']\n",
        "\n",
        "df_mp3000A_1.columns = ['Mes', 'dia', 'Ano', 'H', 'M', 'S', 'Tamb_K', 'Rh_percent', 'Pres_mb', 'Tir_K', 'Rain', 'DataQuality']\n",
        "\n",
        "df_mp3000A_2.columns = ['Mes', 'dia', 'Ano', 'H', 'M', 'S', 'Ch_22_234', 'Ch_22_500', 'Ch_23_034', 'Ch_23_834', 'Ch_25_000', 'Ch_26_234', 'Ch_28_000', 'Ch_30_000', 'Ch_51_248', 'Ch_51_760', 'Ch_52_280', 'Ch_52_804', 'Ch_53_336', 'Ch_53_848', 'Ch_54_400', 'Ch_54_940', 'Ch_55_500', 'Ch_56_020', 'Ch_56_660', 'Ch_57_288', 'Ch_57_964', 'Ch_58_800', 'DataQuality']\n",
        "\n",
        "df_mp3000A_3.columns = ['Mes', 'dia', 'Ano', 'H', 'M', 'S', 'Int_Vapor_cm','Int_Liquid_mm','Cloud_Base_km','DataQuality']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLtnH_5ehkTb"
      },
      "source": [
        "Aqui adicionamos a coluna Datetime para cada um dos Dataframes, com os dados de data e hora. Primeiramente vamos converter o tipo dos dados de data e hora para (int), para que possamos manipulá-los na criação da coluna Datetime, e em seguida adicionaremos a coluna Datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl8I9qeqhnRN"
      },
      "outputs": [],
      "source": [
        "#Modificanto o tipo de dado para (int)\n",
        "df_rd80_joss[['Ano', 'Mes', 'dia', 'H', 'M', 'S']] = df_rd80_joss[['Ano', 'Mes', 'dia', 'H', 'M', 'S']].astype(int)\n",
        "df_mp3000A_1[['Mes', 'dia', 'Ano', 'H', 'M', 'S']] = df_mp3000A_1[['Mes', 'dia', 'Ano', 'H', 'M', 'S']].astype(int)\n",
        "df_mp3000A_2[['Mes', 'dia', 'Ano', 'H', 'M', 'S']] = df_mp3000A_2[['Mes', 'dia', 'Ano', 'H', 'M', 'S']].astype(int)\n",
        "df_mp3000A_3[['Mes', 'dia', 'Ano', 'H', 'M', 'S']] = df_mp3000A_3[['Mes', 'dia', 'Ano', 'H', 'M', 'S']].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fZWtcQMhrKz"
      },
      "source": [
        "Adicionando a coluna Datetime aos Dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzZCno1vhuhb"
      },
      "outputs": [],
      "source": [
        "#Adiciona a coluna Datetime ao Dataframe com os dados do RD80 (Joss)\n",
        "df_rd80_joss['Datetime'] = df_rd80_joss[['Ano', 'Mes', 'dia', 'H', 'M', 'S']].apply(lambda row:\n",
        "                    datetime.datetime(year=row['Ano'], month=row['Mes'],day=row['dia'], hour=row['H'], minute=row[\"M\"], second=row[\"S\"]),axis=1)\n",
        "\n",
        "#Caso queiramos definir a coluna 'Datetime' como Índice\n",
        "#df_rd80_joss = df_rd80_joss.set_index('Datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-fYrlpKh1gb"
      },
      "outputs": [],
      "source": [
        "list_df_mp3000A = [df_mp3000A_1, df_mp3000A_2, df_mp3000A_3]\n",
        "\n",
        "#Loop para percorrer a lista de Dataframes e realizar as manipulações com um único bloco de código.\n",
        "for itemMP3000A in list_df_mp3000A:\n",
        "\n",
        "  #Faz uma busca na coluna Ano e altera o ano de YY para YYYY\n",
        "  itemMP3000A['Ano'][itemMP3000A['Ano'] < 2000] = itemMP3000A['Ano'] + 2000\n",
        "\n",
        "  #Adiciona a coluna Datetime ao Dataframe da vez no Loop for\n",
        "  itemMP3000A['Datetime'] = itemMP3000A[['Mes', 'dia', 'Ano', 'H', 'M', 'S']].apply(lambda row:\n",
        "                    datetime.datetime(year=row['Ano'], month=row['Mes'],day=row['dia'], hour=row['H'], minute=row[\"M\"], second=row[\"S\"]),axis=1)\n",
        "  \n",
        "  #Caso queiramos definir a coluna 'Datetime' como Índice\n",
        "  #itemMP3000A = itemMP3000A.set_index('Datetime')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO3H5KpukbBu"
      },
      "source": [
        "Após os ajustes anteriores, podemos finalizar esta etapa removendo as colunas nas quais não iremos utilizar a partir daqui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVXPzDLckjnu"
      },
      "outputs": [],
      "source": [
        "#Vamos remover as colunas que não iremos utilizar\n",
        "\n",
        "df_rd80_joss = df_rd80_joss.drop(columns=['Mes', 'dia', 'Ano', 'H', 'M', 'S','N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D',\n",
        "       'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D', 'N_D',\n",
        "       'N_D', 'N_D', 'N_D'])\n",
        "\n",
        "df_mp3000A_1 = df_mp3000A_1.drop(columns=['Mes', 'dia', 'Ano', 'H', 'M', 'S', 'Rain'])\n",
        "df_mp3000A_2 = df_mp3000A_2.drop(columns=['Mes', 'dia', 'Ano', 'H', 'M', 'S'])\n",
        "df_mp3000A_3 = df_mp3000A_3.drop(columns=['Mes', 'dia', 'Ano', 'H', 'M', 'S'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pkxjgCLlu_U"
      },
      "source": [
        "Agora vamos juntar os dados do mp3000A em um único Dataframe, utilizando a dunção **merge** do **pandas**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrA61a52l6-E"
      },
      "outputs": [],
      "source": [
        "df_MP3000A = pd.merge(df_mp3000A_1,df_mp3000A_2, how='outer', on = ['Datetime','DataQuality'])\n",
        "df_MP3000A_final = pd.merge(df_MP3000A, df_mp3000A_3, how='outer', on = ['Datetime','DataQuality'])\n",
        "\n",
        "#df_MP3000A_RD80_final = pd.merge(df_rd80_joss, df_MP3000A_final, how='outer', on = ['Datetime'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após juntar os Dataframes, vamos ver o novo tamanho do nosso dataset e quantidade de atributos."
      ],
      "metadata": {
        "id": "PMZj6E8FT2si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tamanho do Dataset\n",
        "print('TAMANHO DO DATASET E QUANTIDADE DE ATRIBUTOS:\\n')\n",
        "\n",
        "print('Tamanho do dataset: ',df_MP3000A_final.shape[0], ' - Quantidade de atributos: ', df_MP3000A_final.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hvZFPipPL_t",
        "outputId": "f1bbce7a-783e-47dc-dc55-b0c64543a400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TAMANHO DO DATASET E QUANTIDADE DE ATRIBUTOS:\n",
            "\n",
            "Tamanho do dataset:  1919064  - Quantidade de atributos:  31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iremos criar um baseline. Uma baseline é importante para ter marcos no projeto. A baseline que iremos criar é por **categoria**, ou seja, se a chuva foi fraca, moderada,forte, muito forte ou se não choveu, tomando como base os dados da medida da distribuição das gotas de chuva que chegam à superfície. tomando como referência o atributo **Rain_Intensity_mm_h** do **RD80**, as regras serão:\n",
        "\n",
        "* Chuva fraca: 0.1-2.5mm/h;\n",
        "* Chuva moderada: >2.5 - 10mm/h;\n",
        "* Chuva forte: >10 - 50mm/h;\n",
        "* Chuva muito forte: >50mm/h\n",
        "* Não choveu: 0mm/h"
      ],
      "metadata": {
        "id": "fJ4bTDXREhZu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX5A-kkc1BI1"
      },
      "source": [
        "Mas antes disso, precisamos aplicar uma pequena normalização. Vamos olhar os dados da coluna Rain_Intensity_mm_h, do Dataframe **df_rd80_joss** e, onde o valor for menor que **0.1** nós iremos substituir por **0 (zero)**. Além disso, também será realizada uma busca por valores nulos (NaN) nos nossos dados e, a correção será também a troca por 0 (zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EGUGRFS1OJM"
      },
      "outputs": [],
      "source": [
        "df_rd80_joss['Rain_Intensity_mm_h'][df_rd80_joss['Rain_Intensity_mm_h'] < 0.1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando e contando os valores Nulos (NaN)\n",
        "df_rd80_joss.isna().sum()"
      ],
      "metadata": {
        "id": "GDLFIdMUduad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando e contando os valores Nulos (NaN)\n",
        "df_MP3000A_final.isna().sum()"
      ],
      "metadata": {
        "id": "pV3pLIATnOF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Substituindo valores Nulos por 0 (zero)\n",
        "df_rd80_joss.fillna(value = 0,  inplace = True) "
      ],
      "metadata": {
        "id": "6bD5XyZKnLjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Substituindo valores Nulos por 0 (zero)\n",
        "df_MP3000A_final.fillna(value = 0,  inplace = True) "
      ],
      "metadata": {
        "id": "LHSq9kX75_RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos verificar se os ajustes nos valores NaN ocorreram corretamente."
      ],
      "metadata": {
        "id": "qxDrkycX64uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando valores Nulos (NaN)\n",
        "df_rd80_joss.isna().sum()"
      ],
      "metadata": {
        "id": "HmjKzQs763LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando valores Nulos (NaN)\n",
        "df_MP3000A_final.isna().sum()"
      ],
      "metadata": {
        "id": "LYCMHCBx63-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJJLQbteAWMP"
      },
      "source": [
        "Agora sim iremos adicionar as colunas para classificação da intensidade de chuva (mm/h) ao Dataframe **df_rd80_joss**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-g5nOsxuebc"
      },
      "outputs": [],
      "source": [
        "#Add as colunas referente à intensidade da chuva. \n",
        "df_rd80_joss=df_rd80_joss.assign(Light_Rain=0, Moderate_Rain=0, Heavy_Rain=0, Very_Heavy_Rain=0, Without_Rain=0, Class_Rain_id=0,Class_Rain='Não choveu')\n",
        "convert_dict = {'Light_Rain': int,'Moderate_Rain': int,'Heavy_Rain': int,'Very_Heavy_Rain': int,\n",
        "                'Without_Rain': int,'Class_Rain_id': int, 'Class_Rain':str}  \n",
        "  \n",
        "df_rd80_joss = df_rd80_joss.astype(convert_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8byl1Yjv8WM"
      },
      "outputs": [],
      "source": [
        "for index, row in df_rd80_joss.iterrows():\n",
        "\n",
        "    if (row['Rain_Intensity_mm_h'] >= 0.1) and (row['Rain_Intensity_mm_h'] <= 2.5):\n",
        "      df_rd80_joss.loc[index,'Light_Rain'] = 1\n",
        "      df_rd80_joss.loc[index,'Class_Rain'] = 'Chuva fraca'\n",
        "      df_rd80_joss.loc[index,'Class_Rain_id'] = 1\n",
        "    elif(row['Rain_Intensity_mm_h']  > 2.5) and (row['Rain_Intensity_mm_h']  <= 10):\n",
        "      df_rd80_joss.loc[index,'Moderate_Rain'] = 1\n",
        "      df_rd80_joss.loc[index,'Class_Rain'] = 'Chuva moderada'\n",
        "      df_rd80_joss.loc[index,'Class_Rain_id'] = 2\n",
        "    elif (row['Rain_Intensity_mm_h']  > 10) and (row['Rain_Intensity_mm_h']  <= 50):\n",
        "      df_rd80_joss.loc[index,'Heavy_Rain'] = 1\n",
        "      df_rd80_joss.loc[index,'Class_Rain'] = 'Chuva forte'\n",
        "      df_rd80_joss.loc[index,'Class_Rain_id'] = 3\n",
        "    elif (row['Rain_Intensity_mm_h']  > 50):\n",
        "      df_rd80_joss.loc[index,'Very_Heavy_Rain'] = 1\n",
        "      df_rd80_joss.loc[index,'Class_Rain'] = 'Chuva muito forte'\n",
        "      df_rd80_joss.loc[index,'Class_Rain_id'] = 4\n",
        "    else:\n",
        "      df_rd80_joss.loc[index,'Without_Rain'] = 1\n",
        "      df_rd80_joss.loc[index,'Class_Rain'] = 'Não choveu'\n",
        "      df_rd80_joss.loc[index,'Class_Rain_id'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz97EICa7w0-"
      },
      "source": [
        "Vamos ajustar os dados referente ao Datetime, adicionando novas colunas, para que possamos manipular durante os filtros na EDA."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_df = [df_rd80_joss, df_MP3000A_final]"
      ],
      "metadata": {
        "id": "tn3zzU5clELR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikfs1peW7zK2"
      },
      "outputs": [],
      "source": [
        "for df_item in list_df:\n",
        "  #Vamos converter o \"Datetime\" em um objeto datetime para que seja mais fácil realizar outras manipulações\n",
        "  df_item['Datetime'] = df_item.Datetime.astype('datetime64')\n",
        "\n",
        "  # Criar uma coluna data\n",
        "  df_item['Date'] = df_item['Datetime'].dt.date\n",
        "\n",
        "  # Criar uma coluna ano\n",
        "  df_item['Year'] = df_item['Datetime'].dt.year\n",
        "\n",
        "  # Criar uma coluna mês\n",
        "  df_item['Month'] = df_item['Datetime'].dt.month\n",
        "  df_item['Month'] = df_item['Month'].apply(lambda x: calendar.month_abbr[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuM82GUf8Js-"
      },
      "outputs": [],
      "source": [
        "for df_item in list_df:\n",
        "  # Ordenando a coluna do mês do ano\n",
        "  df_item['Month'] = pd.Categorical(df_item['Month'], \n",
        "                                  categories= ['Jan','Feb','Mar','Apr','May','Jun', 'Jul',\n",
        "                                              'Aug','Sep','Oct','Nov','Dec'],\n",
        "                                                ordered = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83T1Fccy8QFe"
      },
      "outputs": [],
      "source": [
        "for df_item in list_df:\n",
        "  # Criando uma coluna de dia da semana\n",
        "\n",
        "  df_item['Day_of_week'] = [d.day_name() for d in df_item['Datetime']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGTkKQ4M8aLN"
      },
      "outputs": [],
      "source": [
        "for df_item in list_df:\n",
        "  # Ordenando a coluna do dia da semana\n",
        "\n",
        "  df_item['Day_of_week'] = pd.Categorical(df_item['Day_of_week'], \n",
        "                                  categories= ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday'],\n",
        "                                                ordered = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQhC6yFy8eze"
      },
      "outputs": [],
      "source": [
        "for df_item in list_df:\n",
        "  # Criando a coluna hora do dia\n",
        "\n",
        "  df_item['Time'] = [d.time() for d in df_item['Datetime']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC-hz03k8j19"
      },
      "outputs": [],
      "source": [
        "for df_item in list_df:\n",
        "  # Dividindo a coluna hora do dia em diferentes sessões em um dia\n",
        "\n",
        "  df_item=df_item.assign(session=pd.cut(df_item.Datetime.dt.hour,\n",
        "                              [0,6,12,18,23],\n",
        "                              labels=['Night','Morning','Afternoon','Evening'],\n",
        "                              include_lowest=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos reordenar as colunas dos Dataframes"
      ],
      "metadata": {
        "id": "aiOpWB9qYBCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rd80_joss.columns"
      ],
      "metadata": {
        "id": "h7Ku21Lrl_IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_MP3000A_final.columns"
      ],
      "metadata": {
        "id": "2YmHoIbEXdl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rd80_joss = df_rd80_joss.reindex(columns=['Datetime', 'Date', 'Year', 'Month', 'Day_of_week', 'Time','Rain_Intensity_mm_h', 'radar_reflectivity_1_mm6m3',\n",
        "       'Liquid_watercontent_g_m3', 'Mean_weight_diameter_mm',\n",
        "       'Time_integration_s', 'Light_Rain', 'Moderate_Rain',\n",
        "       'Heavy_Rain', 'Very_Heavy_Rain', 'Without_Rain', 'Class_Rain_id',\n",
        "       'Class_Rain'])"
      ],
      "metadata": {
        "id": "ky_YlHa6X6yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_MP3000A_final = df_MP3000A_final.reindex(columns=['Datetime', 'Date', 'Year', 'Month', 'Day_of_week', 'Time','DataQuality','Tamb_K', 'Rh_percent', 'Pres_mb', 'Tir_K', \n",
        "       'Ch_22_234', 'Ch_22_500', 'Ch_23_034', 'Ch_23_834', 'Ch_25_000',\n",
        "       'Ch_26_234', 'Ch_28_000', 'Ch_30_000', 'Ch_51_248', 'Ch_51_760',\n",
        "       'Ch_52_280', 'Ch_52_804', 'Ch_53_336', 'Ch_53_848', 'Ch_54_400',\n",
        "       'Ch_54_940', 'Ch_55_500', 'Ch_56_020', 'Ch_56_660', 'Ch_57_288',\n",
        "       'Ch_57_964', 'Ch_58_800', 'Int_Vapor_cm', 'Int_Liquid_mm',\n",
        "       'Cloud_Base_km'])"
      ],
      "metadata": {
        "id": "ZdMKqim0wkkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos visualizar novamente as colunas dos Dataframes para verificar se elas forma reordenadas corretamente."
      ],
      "metadata": {
        "id": "ZMk3v_IKxq6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rd80_joss.columns"
      ],
      "metadata": {
        "id": "3TD8eGQoxnT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_MP3000A_final.columns"
      ],
      "metadata": {
        "id": "PLkg7cfBxpxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos garantir de que os nossos dataframes estarão corretamente classificados eem ordem crescente. Vamos usar a função \"**sort_values()**\" do **Pandas**, tomando como referência a coluna \"**Datetime**\"."
      ],
      "metadata": {
        "id": "tWyUoyFXyYe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rd80_joss = df_rd80_joss.sort_values(by=['Datetime'])\n",
        "df_MP3000A_final = df_MP3000A_final.sort_values(by=['Datetime'])"
      ],
      "metadata": {
        "id": "0y9deOWbdSqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualização final dos dados**"
      ],
      "metadata": {
        "id": "Lq_0eo2r0OgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos visualizar as primeiras 2 e últimas 2 linhas dos dados\n",
        "df_rd80_joss.loc[df_rd80_joss['Rain_Intensity_mm_h'] > 0].head(n=2).append(df_rd80_joss.loc[df_rd80_joss['Rain_Intensity_mm_h'] > 0].tail(n=2))"
      ],
      "metadata": {
        "id": "6L8bFs1obobl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos visualizar as primeiras 2 e últimas 2 linhas dos dados\n",
        "df_MP3000A_final.loc[df_MP3000A_final['DataQuality'] > 0].head(n=2).append(df_MP3000A_final.loc[df_MP3000A_final['DataQuality'] > 0].tail(n=2))"
      ],
      "metadata": {
        "id": "jH-Fe7620l_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_9gIK4P86ml"
      },
      "source": [
        "Visualização das estatísticas descritivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebiAVbTd89Y9"
      },
      "outputs": [],
      "source": [
        "#Visualização das estatísticas descritivas\n",
        "df_rd80_joss.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4maJx9UR9Bdl"
      },
      "outputs": [],
      "source": [
        "#Visualização das estatísticas descritivas\n",
        "df_MP3000A_final.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após todos os ajustes anteriores, finalmente podemos exportar/salvar nossos Dataframes para o formato CSV, afim de usá-los na próxima etapa deste trabalho: **Exploratory Data Analysis (EDA)**."
      ],
      "metadata": {
        "id": "6FvlLOegzFng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Salvando/Exportando os Dataframes para o formato CSV\n",
        "df_rd80_joss.to_csv('./df_RD80_JOSS_final.csv')\n",
        "df_MP3000A_final.to_csv('./df_MP3000A_final.csv')"
      ],
      "metadata": {
        "id": "ojisLEFwQI-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33jPwgUyd9iL"
      },
      "source": [
        "# **Referências**\n",
        "\n",
        "Wuerges, Artur Filipe Ewald e Borba, José AlonsoRedes neurais, lógica nebulosa e algoritmos genéticos: aplicações e possibilidades em finanças e contabilidade. JISTEM - Journal of Information Systems and Technology Management [online]. 2010, v. 7, n. 1 [Acessado 9 Novembro 2022] , pp. 163-182. Disponível em: <https://doi.org/10.4301/S1807-17752010000100007>. Epub 22 Nov 2010. ISSN 1807-1775. https://doi.org/10.4301/S1807-17752010000100007."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}